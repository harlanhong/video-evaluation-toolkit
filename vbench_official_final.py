#!/usr/bin/env python3
"""
VBench Direct Integration - ç›´æ¥é›†æˆVBenchæ ¸å¿ƒé€»è¾‘
ç›´æ¥ä½¿ç”¨VBenchç±»ï¼Œç¡®ä¿100%ä¸€è‡´æ€§

Copyright (c) 2025 Fating Hong <fatinghong@gmail.com>
All rights reserved.

This module provides direct integration with VBench for video generation quality assessment.
"""

import torch
import os
import json
import tempfile
import shutil
from datetime import datetime
from typing import List, Dict, Optional
import argparse

# å¯¼å…¥VBench
from vbench import VBench
from vbench.distributed import dist_init, print0


class VBenchDirect:
    """VBench Direct - ç›´æ¥ä½¿ç”¨VBenchç±»"""
    
    def __init__(self, device: str = "cuda", cache_dir: str = "./cache"):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        print(f"ğŸš€ åˆå§‹åŒ–VBench Direct (è®¾å¤‡: {self.device})")
        
        # å®šä¹‰6ä¸ªæ ¸å¿ƒæŒ‡æ ‡
        self.core_metrics = [
            'subject_consistency',
            'background_consistency', 
            'motion_smoothness',
            'dynamic_degree',
            'aesthetic_quality',
            'imaging_quality'
        ]
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        dist_init()
        
        # åˆ›å»ºä¸´æ—¶çš„jsoné…ç½®æ–‡ä»¶
        self.full_json_path = self._create_temp_json()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_path = tempfile.mkdtemp(prefix="vbench_output_")
        
        # åˆå§‹åŒ–VBenchå¯¹è±¡
        self.vbench = VBench(self.device, self.full_json_path, self.output_path)
        
        print(f"âœ… VBenchå¯¹è±¡åˆå§‹åŒ–å®Œæˆ")
    
    def _create_temp_json(self) -> str:
        """åˆ›å»ºä¸´æ—¶çš„VBenché…ç½®JSONæ–‡ä»¶"""
        # ç®€åŒ–çš„VBenché…ç½®
        config = {
            "subject_consistency": {},
            "background_consistency": {},
            "motion_smoothness": {},
            "dynamic_degree": {},
            "aesthetic_quality": {},
            "imaging_quality": {}
        }
        
        temp_json = os.path.join(self.cache_dir, "temp_vbench_config.json")
        with open(temp_json, 'w') as f:
            json.dump(config, f, indent=2)
        
        return temp_json
    
    def _prepare_video_directory(self, video_paths: List[str]) -> str:
        """å‡†å¤‡è§†é¢‘ç›®å½•"""
        temp_dir = tempfile.mkdtemp(prefix="vbench_videos_")
        
        print(f"ğŸ”„ å‡†å¤‡è§†é¢‘ç›®å½•: {temp_dir}")
        
        for i, video_path in enumerate(video_paths):
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            _, ext = os.path.splitext(video_path)
            dest_name = f"video_{i:03d}{ext}"
            dest_path = os.path.join(temp_dir, dest_name)
            
            print(f"   ğŸ“„ å¤åˆ¶: {os.path.basename(video_path)} -> {dest_name}")
            shutil.copy2(video_path, dest_path)
        
        return temp_dir
    
    def evaluate_videos(self, video_paths: List[str], metrics: Optional[List[str]] = None) -> Dict[str, float]:
        """è¯„ä¼°è§†é¢‘ï¼Œç›´æ¥è°ƒç”¨VBenchæ ¸å¿ƒé€»è¾‘"""
        if metrics is None:
            metrics = self.core_metrics.copy()
        
        print(f"ğŸ” å¼€å§‹è¯„ä¼° {len(video_paths)} ä¸ªè§†é¢‘ï¼Œå…± {len(metrics)} ä¸ªæŒ‡æ ‡")
        print("=" * 60)
        
        temp_video_dir = None
        
        try:
            # å‡†å¤‡è§†é¢‘ç›®å½•
            temp_video_dir = self._prepare_video_directory(video_paths)
            
            # === æ ¸å¿ƒVBenché€»è¾‘ (ä»å®˜æ–¹evaluate.pyç§»æ¤) ===
            print0(f'ğŸš€ å¼€å§‹VBenchè¯„ä¼°')
            
            current_time = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')
            
            kwargs = {}
            prompt = []  # ç©ºåˆ—è¡¨è¡¨ç¤ºä»æ–‡ä»¶åè¯»å–prompt
            
            # è®¾ç½®å‚æ•°
            kwargs['imaging_quality_preprocessing_mode'] = 'longer'
            
            # è°ƒç”¨VBenchæ ¸å¿ƒè¯„ä¼°å‡½æ•°
            print0(f'ğŸ“Š è°ƒç”¨VBench.evaluate...')
            
            self.vbench.evaluate(
                videos_path=temp_video_dir,
                name=f'results_{current_time}',
                prompt_list=prompt,  # ä»æ–‡ä»¶åè¯»å–prompt
                dimension_list=metrics,
                local=False,  # ä¸ä»æœ¬åœ°åŠ è½½æ£€æŸ¥ç‚¹
                read_frame=False,  # ç›´æ¥è¯»å–è§†é¢‘
                mode='custom_input',
                **kwargs
            )
            
            print0('âœ… VBenchè¯„ä¼°å®Œæˆ')
            
            # è§£æç»“æœ
            results = self._parse_results(current_time)
            
            return results
            
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if temp_video_dir and os.path.exists(temp_video_dir):
                shutil.rmtree(temp_video_dir)
                print(f"ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶è§†é¢‘ç›®å½•: {temp_video_dir}")
    
    def _parse_results(self, result_name: str) -> Dict[str, float]:
        """è§£æVBenchç»“æœ"""
        print(f"ğŸ“Š è§£æVBenchç»“æœ...")
        
        # æŸ¥æ‰¾eval_resultsæ–‡ä»¶ (è¿™ä¸ªæ–‡ä»¶åŒ…å«è¯„ä¼°ç»“æœ)
        result_file = None
        
        for file in os.listdir(self.output_path):
            if file.endswith("_eval_results.json") and result_name in file:
                result_file = os.path.join(self.output_path, file)
                break
        
        if result_file is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°eval_resultsæ–‡ä»¶åœ¨: {self.output_path}")
            print(f"ç›®å½•å†…å®¹: {os.listdir(self.output_path)}")
            # è¿”å›é»˜è®¤å€¼
            return {metric: 0.0 for metric in self.core_metrics}
        
        print(f"   ğŸ“„ ç»“æœæ–‡ä»¶: {result_file}")
        
        with open(result_file, 'r', encoding='utf-8') as f:
            raw_results = json.load(f)
        
        print(f"   ğŸ“‹ åŸå§‹ç»“æœé”®: {list(raw_results.keys())}")
        
        # è§£æç»“æœ
        # VBenchæ ¼å¼: {"metric_name": [total_score, detailed_results]}
        results = {}
        
        for metric in self.core_metrics:
            if metric in raw_results:
                metric_data = raw_results[metric]
                
                if isinstance(metric_data, list) and len(metric_data) >= 1:
                    # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ€»åˆ†
                    score = metric_data[0]
                    if isinstance(score, (int, float)):
                        results[metric] = float(score)
                        print(f"   âœ… {metric}: {results[metric]}")
                    else:
                        print(f"   âš ï¸ {metric} ç¬¬ä¸€ä¸ªå…ƒç´ ä¸æ˜¯æ•°å€¼: {type(score)}")
                        results[metric] = 0.0
                        
                elif isinstance(metric_data, (int, float)):
                    results[metric] = float(metric_data)
                    print(f"   âœ… {metric}: {results[metric]}")
                else:
                    print(f"   âš ï¸ æœªçŸ¥çš„ {metric} æ ¼å¼: {type(metric_data)}")
                    results[metric] = 0.0
            else:
                print(f"   âŒ ç»“æœä¸­æœªæ‰¾åˆ°: {metric}")
                results[metric] = 0.0
        
        return results
    
    def save_results(self, results: Dict[str, float], output_path: str = "vbench_direct_results.json"):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        clean_results = {}
        for k, v in results.items():
            if isinstance(v, (int, float)):
                clean_results[k] = float(v)
            else:
                clean_results[k] = v
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if hasattr(self, 'output_path') and os.path.exists(self.output_path):
            shutil.rmtree(self.output_path)
            print(f"ğŸ—‘ï¸  æ¸…ç†è¾“å‡ºç›®å½•: {self.output_path}")
        
        if hasattr(self, 'full_json_path') and os.path.exists(self.full_json_path):
            os.remove(self.full_json_path)
            print(f"ğŸ—‘ï¸  æ¸…ç†é…ç½®æ–‡ä»¶: {self.full_json_path}")


def main():
    parser = argparse.ArgumentParser(description="VBench Direct - ç›´æ¥é›†æˆVBenchæ ¸å¿ƒé€»è¾‘")
    parser.add_argument("--videos", type=str, required=True, help="è§†é¢‘æ–‡ä»¶è·¯å¾„(ç”¨é€—å·åˆ†éš”å¤šä¸ªæ–‡ä»¶)")
    parser.add_argument("--metrics", type=str, nargs='+', choices=['subject_consistency', 'background_consistency', 'motion_smoothness', 'dynamic_degree', 'aesthetic_quality', 'imaging_quality', 'all'], default=['all'], help="è¦è®¡ç®—çš„æŒ‡æ ‡")
    parser.add_argument("--device", type=str, default="cuda", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--output", type=str, default="vbench_direct_results.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è§£æè§†é¢‘è·¯å¾„
    video_paths = [path.strip() for path in args.videos.split(',')]
    
    # éªŒè¯è§†é¢‘æ–‡ä»¶å­˜åœ¨
    for video_path in video_paths:
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return
    
    # è§£ææŒ‡æ ‡
    if 'all' in args.metrics:
        metrics = None  # ä½¿ç”¨é»˜è®¤çš„6ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    else:
        metrics = args.metrics
    
    # åˆå§‹åŒ–VBench Direct
    vbench_direct = VBenchDirect(device=args.device)
    
    try:
        # è¯„ä¼°è§†é¢‘
        results = vbench_direct.evaluate_videos(video_paths, metrics)
        
        # ä¿å­˜ç»“æœ
        vbench_direct.save_results(results, args.output)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nï¿½ï¿½ VBench Direct ç»“æœ:")
        for metric, score in results.items():
            print(f"  {metric}: {score}")
            
    finally:
        # æ¸…ç†
        vbench_direct.cleanup()


if __name__ == "__main__":
    main()
