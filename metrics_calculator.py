#!/usr/bin/env python3
"""
ç»¼åˆè§†é¢‘æŒ‡æ ‡è®¡ç®—å™¨ (VBenché›†æˆç‰ˆæœ¬)
æ•´åˆäº†LSEè®¡ç®—ã€VBenchæŒ‡æ ‡å’Œå…¶ä»–è§†é¢‘è´¨é‡æŒ‡æ ‡

Copyright (c) 2025 Fating Hong <fatinghong@gmail.com>
All rights reserved.

This module integrates VBench metrics with comprehensive video evaluation tools.

æ”¯æŒçš„æŒ‡æ ‡:
- è§†é¢‘åŸºæœ¬ä¿¡æ¯ (ä¸éœ€è¦GT): å¸§æ•°ã€åˆ†è¾¨ç‡ã€å¸§ç‡ã€æ—¶é•¿
- å›¾åƒç»Ÿè®¡ä¿¡æ¯ (ä¸éœ€è¦GT): äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦ã€æ¸…æ™°åº¦
- äººè„¸åˆ†æ (ä¸éœ€è¦GT): äººè„¸æ£€æµ‹ç‡ã€å¹³å‡äººè„¸å¤§å°ã€äººè„¸ç¨³å®šæ€§
- è¿åŠ¨åˆ†æ (ä¸éœ€è¦GT): è¿åŠ¨å¼ºåº¦ã€å¸§é—´å·®å¼‚
- äººè„¸åŒºåŸŸå›¾åƒè´¨é‡ (éœ€è¦GT): face_psnr, face_ssim, face_lpips
- å”‡åŒæ­¥æŒ‡æ ‡ (ä¸éœ€è¦GT): LSE distance, LSE confidence
- VBenchæŒ‡æ ‡ (ä¸éœ€è¦GT): subject_consistency, background_consistency, motion_smoothness, 
  dynamic_degree, aesthetic_quality, imaging_quality

ä½¿ç”¨æ–¹æ³•:
    from evalutation.metrics_calculator import VideoMetricsCalculator
    
    # ä¸åŒ…å«VBench
    calculator = VideoMetricsCalculator()
    metrics = calculator.calculate_video_metrics("video.mp4")
    
    # åŒ…å«VBench
    calculator = VideoMetricsCalculator(enable_vbench=True)
    metrics = calculator.calculate_video_metrics("video.mp4")
"""

import os
import sys
import time
import cv2
import numpy as np
import torch
import tempfile
import shutil
import glob
import json
import argparse
from typing import Optional, Dict, List, Tuple, Any
from pathlib import Path
from tqdm import tqdm

# å›¾åƒè´¨é‡æŒ‡æ ‡
import lpips
from skimage.metrics import peak_signal_noise_ratio, structural_similarity

# LSEè®¡ç®—å™¨
try:
    # ä½œä¸ºåŒ…å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .lse_calculator import LSECalculator
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)
    from lse_calculator import LSECalculator

# VBenchè®¡ç®—å™¨
try:
    # ä½œä¸ºåŒ…å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .vbench_official_final import VBenchDirect
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)
    from vbench_official_final import VBenchDirect


class VideoMetricsCalculator:
    """ç»¼åˆè§†é¢‘æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, device: str = "cuda", enable_vbench: bool = False):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡ ("cuda" æˆ– "cpu")
            enable_vbench: æ˜¯å¦å¯ç”¨VBenchæŒ‡æ ‡è®¡ç®—
        """
        self.device = device if torch.cuda.is_available() else "cpu"
        self.enable_vbench = enable_vbench
        
        # åˆå§‹åŒ–LPIPSæ¨¡å‹
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–LPIPSæ¨¡å‹...")
        self.lpips_fn = lpips.LPIPS(net='alex').to(self.device)
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨...")
        self.face_detection_method = self._initialize_face_detector()
        self.face_detection_available = self.face_detection_method is not None
        
        if self.face_detection_available:
            print(f"âœ… äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ (æ–¹æ³•: {self.face_detection_method})")
        else:
            print("âš ï¸ æ‰€æœ‰äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡äººè„¸ç›¸å…³æŒ‡æ ‡")
        
        # åˆå§‹åŒ–LSEè®¡ç®—å™¨
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–LSEè®¡ç®—å™¨...")
        try:
            self.lse_calculator = LSECalculator(device=self.device)
            self.lse_available = True
        except Exception as e:
            print(f"âš ï¸ LSEè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.lse_available = False
        
        # åˆå§‹åŒ–VBenchè®¡ç®—å™¨
        if self.enable_vbench:
            print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–VBenchè®¡ç®—å™¨...")
            try:
                self.vbench_calculator = VBenchDirect(device=self.device)
                self.vbench_available = True
                print("âœ… VBenchè®¡ç®—å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ VBenchè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.vbench_available = False
        else:
            self.vbench_available = False
        
        print(f"ğŸš€ æŒ‡æ ‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device}, VBench: {self.vbench_available})")
    
    def calculate_video_metrics(self, 
                               pred_path: str, 
                               gt_path: Optional[str] = None,
                               audio_path: Optional[str] = None) -> Dict[str, Any]:
        """
        è®¡ç®—å•ä¸ªè§†é¢‘çš„æ‰€æœ‰æŒ‡æ ‡
        
        Args:
            pred_path: é¢„æµ‹è§†é¢‘è·¯å¾„
            gt_path: çœŸå€¼è§†é¢‘è·¯å¾„ (å¯é€‰)
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œå·²å¼ƒç”¨)
            
        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        
        # åˆå§‹åŒ–æŒ‡æ ‡å­—å…¸
        metrics = {
            'video_path': pred_path,
            'has_ground_truth': gt_path is not None and os.path.exists(gt_path) if gt_path else False,
            'has_audio': False,  # ä¸å†éœ€è¦å¤–éƒ¨éŸ³é¢‘æ–‡ä»¶
            'vbench_enabled': self.vbench_available,
            
            # åŸºæœ¬ä¿¡æ¯ï¼ˆä¸éœ€è¦ground truthï¼‰
            'frame_count': 0,
            'width': 0,
            'height': 0,
            'fps': 0.0,
            'duration_seconds': 0.0,
            
            # å›¾åƒç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸éœ€è¦ground truthï¼‰
            'mean_brightness': 0.0,
            'mean_contrast': 0.0,
            'mean_saturation': 0.0,
            'sharpness_score': 0.0,
            
            # äººè„¸æ£€æµ‹ç»Ÿè®¡ï¼ˆä¸éœ€è¦ground truthï¼‰
            'face_detection_rate': 0.0,  # æ£€æµ‹åˆ°äººè„¸çš„å¸§æ¯”ä¾‹
            'avg_face_size': 0.0,        # å¹³å‡äººè„¸å¤§å°
            'face_stability': 0.0,       # äººè„¸ä½ç½®ç¨³å®šæ€§
            
            # è¿åŠ¨åˆ†æï¼ˆä¸éœ€è¦ground truthï¼‰
            'motion_intensity': 0.0,     # è¿åŠ¨å¼ºåº¦
            'frame_difference': 0.0,     # å¹³å‡å¸§é—´å·®å¼‚
            
            # éœ€è¦ground truthçš„æŒ‡æ ‡ï¼ˆåªè®¡ç®—äººè„¸åŒºåŸŸï¼‰
            'face_psnr': None,        # äººè„¸åŒºåŸŸPSNR
            'face_ssim': None,        # äººè„¸åŒºåŸŸSSIM  
            'face_lpips': None,       # äººè„¸åŒºåŸŸLPIPS
            
            # å”‡åŒæ­¥æŒ‡æ ‡ï¼ˆä½¿ç”¨LSEè®¡ç®—å™¨ï¼Œä¸éœ€è¦å¤–éƒ¨éŸ³é¢‘ï¼‰
            'lse_distance': None,     # LSEè·ç¦»åˆ†æ•°
            'lse_confidence': None,   # LSEç½®ä¿¡åº¦åˆ†æ•°
            
            # VBenchæŒ‡æ ‡ï¼ˆä¸éœ€è¦ground truthï¼‰
            'subject_consistency': None,     # ä¸»ä½“ä¸€è‡´æ€§
            'background_consistency': None,  # èƒŒæ™¯ä¸€è‡´æ€§
            'motion_smoothness': None,       # è¿åŠ¨å¹³æ»‘æ€§
            'dynamic_degree': None,          # åŠ¨æ€ç¨‹åº¦
            'aesthetic_quality': None,       # ç¾å­¦è´¨é‡
            'imaging_quality': None,         # æˆåƒè´¨é‡
            
            'error': None
        }
        
        try:
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶
            pred_cap = cv2.VideoCapture(pred_path)
            if not pred_cap.isOpened():
                metrics['error'] = f"æ— æ³•æ‰“å¼€é¢„æµ‹è§†é¢‘: {pred_path}"
                return metrics
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            fps = pred_cap.get(cv2.CAP_PROP_FPS)
            width = int(pred_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(pred_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            metrics['fps'] = fps
            metrics['width'] = width
            metrics['height'] = height
            
            pred_frames = []
            face_detections = []
            brightness_values = []
            contrast_values = []
            saturation_values = []
            sharpness_values = []
            
            while True:
                ret, frame = pred_cap.read()
                if not ret:
                    break
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pred_frames.append(frame_rgb)
                
                # è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
                gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)
                
                # äº®åº¦ï¼ˆç°åº¦å‡å€¼ï¼‰
                brightness = np.mean(gray)
                brightness_values.append(brightness)
                
                # å¯¹æ¯”åº¦ï¼ˆç°åº¦æ ‡å‡†å·®ï¼‰
                contrast = np.std(gray)
                contrast_values.append(contrast)
                
                # é¥±å’Œåº¦ï¼ˆHSVç©ºé—´çš„Sé€šé“å‡å€¼ï¼‰
                hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)
                saturation = np.mean(hsv[:, :, 1])
                saturation_values.append(saturation)
                
                # æ¸…æ™°åº¦ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼‰
                laplacian = cv2.Laplacian(gray, cv2.CV_64F)
                sharpness = laplacian.var()
                sharpness_values.append(sharpness)
                
                # äººè„¸æ£€æµ‹
                if self.face_detection_available:
                    face_bbox = self.detect_face_bbox(frame_rgb)
                    if face_bbox is not None:
                        x, y, w, h = face_bbox
                        face_size = w * h
                        face_detections.append({'frame_idx': len(pred_frames)-1, 'bbox': face_bbox, 'size': face_size})
                
            pred_cap.release()
            
            frame_count = len(pred_frames)
            metrics['frame_count'] = frame_count
            metrics['duration_seconds'] = frame_count / fps if fps > 0 else 0
            
            # è®¡ç®—ä¸éœ€è¦ground truthçš„æŒ‡æ ‡
            if brightness_values:
                metrics['mean_brightness'] = np.mean(brightness_values)
                metrics['mean_contrast'] = np.mean(contrast_values)
                metrics['mean_saturation'] = np.mean(saturation_values)
                metrics['sharpness_score'] = np.mean(sharpness_values)
            
            # äººè„¸æ£€æµ‹ç»Ÿè®¡
            if face_detections:
                metrics['face_detection_rate'] = len(face_detections) / frame_count
                metrics['avg_face_size'] = np.mean([fd['size'] for fd in face_detections])
                
                # è®¡ç®—äººè„¸ä½ç½®ç¨³å®šæ€§ï¼ˆç›¸é‚»å¸§äººè„¸ä¸­å¿ƒç‚¹è·ç¦»çš„æ–¹å·®ï¼‰
                if len(face_detections) > 1:
                    centers = []
                    for fd in face_detections:
                        x, y, w, h = fd['bbox']
                        center_x = x + w/2
                        center_y = y + h/2
                        centers.append((center_x, center_y))
                    
                    distances = []
                    for i in range(1, len(centers)):
                        dist = np.sqrt((centers[i][0] - centers[i-1][0])**2 + 
                                    (centers[i][1] - centers[i-1][1])**2)
                        distances.append(dist)
                    
                    # ç¨³å®šæ€§ = 1 / (1 + å¹³å‡è·ç¦»)ï¼Œå€¼è¶Šå¤§è¶Šç¨³å®š
                    metrics['face_stability'] = 1.0 / (1.0 + np.mean(distances)) if distances else 1.0
            
            # è¿åŠ¨åˆ†æ
            if len(pred_frames) > 1:
                frame_diffs = []
                for i in range(1, len(pred_frames)):
                    diff = np.mean(np.abs(pred_frames[i].astype(float) - pred_frames[i-1].astype(float)))
                    frame_diffs.append(diff)
                
                metrics['motion_intensity'] = np.std(frame_diffs) if frame_diffs else 0.0
                metrics['frame_difference'] = np.mean(frame_diffs) if frame_diffs else 0.0
            
            # è®¡ç®—LSEåˆ†æ•°ï¼ˆä½¿ç”¨LSEè®¡ç®—å™¨ï¼‰
            if self.lse_available:
                print(f"ğŸµ è®¡ç®—LSEåˆ†æ•° (ä½¿ç”¨LSEè®¡ç®—å™¨)")
                try:
                    lse_distance, lse_confidence = self.lse_calculator.calculate_single_video(pred_path, verbose=False)
                    metrics['lse_distance'] = lse_distance
                    metrics['lse_confidence'] = lse_confidence
                except Exception as e:
                    print(f"âš ï¸ LSEè®¡ç®—å¤±è´¥: {e}")
                    metrics['lse_distance'] = None
                    metrics['lse_confidence'] = None
            else:
                print(f"âš ï¸ LSEè®¡ç®—å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡LSEè®¡ç®—")
            
            # è®¡ç®—VBenchåˆ†æ•°
            if self.vbench_available:
                print(f"ğŸ”¥ è®¡ç®—VBenchæŒ‡æ ‡ (6ä¸ªæ ¸å¿ƒæŒ‡æ ‡)")
                try:
                    vbench_results = self.vbench_calculator.evaluate_videos([pred_path])
                    
                    # å°†VBenchç»“æœåˆå¹¶åˆ°metricsä¸­
                    for metric_name in ['subject_consistency', 'background_consistency', 'motion_smoothness', 
                                      'dynamic_degree', 'aesthetic_quality', 'imaging_quality']:
                        if metric_name in vbench_results:
                            metrics[metric_name] = vbench_results[metric_name]
                            print(f"   âœ… {metric_name}: {metrics[metric_name]}")
                        else:
                            print(f"   âŒ VBenchç»“æœä¸­æœªæ‰¾åˆ°: {metric_name}")
                            metrics[metric_name] = None
                    
                except Exception as e:
                    print(f"âš ï¸ VBenchè®¡ç®—å¤±è´¥: {e}")
                    for metric_name in ['subject_consistency', 'background_consistency', 'motion_smoothness', 
                                      'dynamic_degree', 'aesthetic_quality', 'imaging_quality']:
                        metrics[metric_name] = None
            else:
                print(f"âš ï¸ VBenchè®¡ç®—å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡VBenchè®¡ç®—")
            
            # å¦‚æœæœ‰çœŸå€¼è§†é¢‘ï¼Œè®¡ç®—å¯¹æ¯”æŒ‡æ ‡ï¼ˆåªè®¡ç®—äººè„¸åŒºåŸŸï¼‰
            if gt_path and os.path.exists(gt_path):
                print(f"ğŸ” è®¡ç®—äººè„¸åŒºåŸŸå¯¹æ¯”æŒ‡æ ‡ (ä¸çœŸå€¼å¯¹æ¯”)")
                gt_cap = cv2.VideoCapture(gt_path)
                if gt_cap.isOpened():
                    gt_frames = []
                    while True:
                        ret, frame = gt_cap.read()
                        if not ret:
                            break
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        gt_frames.append(frame_rgb)
                    gt_cap.release()
                    
                    # è®¡ç®—å¸§çº§åˆ«æŒ‡æ ‡
                    frame_metrics_list = []
                    min_frames = min(len(pred_frames), len(gt_frames))
                    
                    for i in range(min_frames):
                        frame_metrics = self.calculate_frame_metrics(pred_frames[i], gt_frames[i])
                        if frame_metrics:
                            frame_metrics_list.append(frame_metrics)
                    
                    if frame_metrics_list:
                        # è®¡ç®—å¹³å‡å€¼
                        for key in ['face_psnr', 'face_ssim', 'face_lpips']:
                            values = [fm.get(key, 0.0) for fm in frame_metrics_list if fm.get(key, 0.0) > 0]
                            if values:
                                metrics[key] = np.mean(values)
                else:
                    print(f"âš ï¸ æ— æ³•æ‰“å¼€çœŸå€¼è§†é¢‘: {gt_path}")
            else:
                print(f"âš ï¸ æ— çœŸå€¼è§†é¢‘ï¼Œè·³è¿‡å¯¹æ¯”æŒ‡æ ‡è®¡ç®—")
        
        except Exception as e:
            metrics['error'] = str(e)
            print(f"è®¡ç®—è§†é¢‘æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        
        return metrics
    
    def calculate_frame_metrics(self, pred_frame: np.ndarray, gt_frame: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å•å¸§æŒ‡æ ‡ï¼ˆåªè®¡ç®—äººè„¸åŒºåŸŸï¼‰"""
        
        # æ£€æµ‹äººè„¸åŒºåŸŸ
        pred_face_bbox = self.detect_face_bbox(pred_frame)
        gt_face_bbox = self.detect_face_bbox(gt_frame)
        
        metrics = {
            'face_psnr': 0.0,
            'face_ssim': 0.0,
            'face_lpips': 0.0
        }
        
        # å¦‚æœä¸¤å¸§éƒ½æ£€æµ‹åˆ°äººè„¸ï¼Œè®¡ç®—äººè„¸åŒºåŸŸæŒ‡æ ‡
        if pred_face_bbox is not None and gt_face_bbox is not None:
            # ä½¿ç”¨è¾ƒå¤§çš„bboxç¡®ä¿åŒ…å«å®Œæ•´äººè„¸
            x1 = min(pred_face_bbox[0], gt_face_bbox[0])
            y1 = min(pred_face_bbox[1], gt_face_bbox[1])
            x2 = max(pred_face_bbox[0] + pred_face_bbox[2], gt_face_bbox[0] + gt_face_bbox[2])
            y2 = max(pred_face_bbox[1] + pred_face_bbox[3], gt_face_bbox[1] + gt_face_bbox[3])
            
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            h, w = pred_frame.shape[:2]
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)
            
            if x2 > x1 and y2 > y1:  # ç¡®ä¿æœ‰æ•ˆåŒºåŸŸ
                pred_face = pred_frame[y1:y2, x1:x2]
                gt_face = gt_frame[y1:y2, x1:x2]
                
                # è®¡ç®—äººè„¸åŒºåŸŸPSNR
                try:
                    face_psnr = peak_signal_noise_ratio(gt_face, pred_face, data_range=255)
                    metrics['face_psnr'] = face_psnr
                except:
                    metrics['face_psnr'] = 0.0
                
                # è®¡ç®—äººè„¸åŒºåŸŸSSIM
                try:
                    face_ssim = structural_similarity(
                        gt_face, pred_face, 
                        multichannel=True, 
                        data_range=255,
                        channel_axis=-1
                    )
                    metrics['face_ssim'] = face_ssim
                except:
                    metrics['face_ssim'] = 0.0
                
                # è®¡ç®—äººè„¸åŒºåŸŸLPIPS
                try:
                    # è½¬æ¢ä¸ºtensoræ ¼å¼ [1, C, H, W]
                    pred_tensor = torch.from_numpy(pred_face).permute(2, 0, 1).unsqueeze(0).float() / 255.0
                    gt_tensor = torch.from_numpy(gt_face).permute(2, 0, 1).unsqueeze(0).float() / 255.0
                    
                    # å½’ä¸€åŒ–åˆ°[-1, 1]
                    pred_tensor = pred_tensor * 2.0 - 1.0
                    gt_tensor = gt_tensor * 2.0 - 1.0
                    
                    pred_tensor = pred_tensor.to(self.device)
                    gt_tensor = gt_tensor.to(self.device)
                    
                    face_lpips = self.lpips_fn(pred_tensor, gt_tensor).item()
                    metrics['face_lpips'] = face_lpips
                except:
                    metrics['face_lpips'] = 0.0
        
        return metrics
    
    def _initialize_face_detector(self) -> Optional[str]:
        """åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒæ–¹æ³•"""
        
        # æ–¹æ³•1: MediaPipe Face Detection (æ¨è)
        try:
            import mediapipe as mp
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_drawing = mp.solutions.drawing_utils
            self.face_detector_mp = self.mp_face_detection.FaceDetection(
                model_selection=0, min_detection_confidence=0.5
            )
            print("   ğŸš€ ä½¿ç”¨MediaPipeäººè„¸æ£€æµ‹ (æ¨è)")
            return "mediapipe"
        except ImportError:
            print("   âš ï¸ MediaPipeæœªå®‰è£…ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
        except Exception as e:
            print(f"   âš ï¸ MediaPipeåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ–¹æ³•2: YOLOv8 Face Detection
        try:
            from ultralytics import YOLO
            # å°è¯•åŠ è½½YOLOv8n-faceæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            self.yolo_model = YOLO('yolov8n-face.pt')
            print("   âš¡ ä½¿ç”¨YOLOv8äººè„¸æ£€æµ‹")
            return "yolov8"
        except ImportError:
            print("   âš ï¸ ultralyticsæœªå®‰è£…ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
        except Exception as e:
            print(f"   âš ï¸ YOLOv8åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ–¹æ³•3: OpenCV DNN Face Detection
        try:
            # ä½¿ç”¨OpenCV DNNæ¨¡å—åŠ è½½é¢„è®­ç»ƒçš„äººè„¸æ£€æµ‹æ¨¡å‹
            # è¿™é‡Œå¯ä»¥ä½¿ç”¨SSD MobileNetæˆ–ResNetæ¨¡å‹
            self.face_net = cv2.dnn.readNetFromTensorflow(
                # éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶
                'opencv_face_detector_uint8.pb',
                'opencv_face_detector.pbtxt'
            )
            print("   ğŸ”§ ä½¿ç”¨OpenCV DNNäººè„¸æ£€æµ‹")
            return "opencv_dnn"
        except Exception as e:
            print(f"   âš ï¸ OpenCV DNNåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ–¹æ³•4: ä¼ ç»ŸHaarçº§è” (fallback)
        try:
            self.face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            print("   ğŸ“° ä½¿ç”¨ä¼ ç»ŸHaarçº§è”åˆ†ç±»å™¨ (fallback)")
            return "haar_cascade"
        except Exception as e:
            print(f"   âŒ Haarçº§è”åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return None
    
    def detect_face_bbox(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """æ£€æµ‹äººè„¸è¾¹ç•Œæ¡† - æ”¯æŒå¤šç§ç°ä»£æ£€æµ‹å™¨"""
        if not self.face_detection_available:
            return None
        
        h, w = frame.shape[:2]
        
        try:
            if self.face_detection_method == "mediapipe":
                return self._detect_face_mediapipe(frame, h, w)
            elif self.face_detection_method == "yolov8":
                return self._detect_face_yolov8(frame, h, w)
            elif self.face_detection_method == "opencv_dnn":
                return self._detect_face_opencv_dnn(frame, h, w)
            elif self.face_detection_method == "haar_cascade":
                return self._detect_face_haar_cascade(frame, h, w)
        except Exception as e:
            print(f"âš ï¸ äººè„¸æ£€æµ‹å¤±è´¥ ({self.face_detection_method}): {e}")
            return None
        
        return None
    
    def _detect_face_mediapipe(self, frame: np.ndarray, h: int, w: int) -> Optional[Tuple[int, int, int, int]]:
        """ä½¿ç”¨MediaPipeæ£€æµ‹äººè„¸"""
        # frameå·²ç»æ˜¯RGBæ ¼å¼ï¼Œä¸éœ€è¦è½¬æ¢
        results = self.face_detector_mp.process(frame)
        
        if results.detections:
            # è·å–ç½®ä¿¡åº¦æœ€é«˜çš„äººè„¸
            best_detection = max(results.detections, key=lambda d: d.score[0])
            bbox = best_detection.location_data.relative_bounding_box
            
            # è½¬æ¢ä¸ºç»å¯¹åæ ‡
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x = max(0, min(x, w - 1))
            y = max(0, min(y, h - 1))
            width = max(1, min(width, w - x))
            height = max(1, min(height, h - y))
            
            return (x, y, width, height)
        
        return None
    
    def _detect_face_yolov8(self, frame: np.ndarray, h: int, w: int) -> Optional[Tuple[int, int, int, int]]:
        """ä½¿ç”¨YOLOv8æ£€æµ‹äººè„¸"""
        results = self.yolo_model(frame, verbose=False)
        
        if len(results) > 0 and len(results[0].boxes) > 0:
            # è·å–ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            boxes = results[0].boxes
            confidences = boxes.conf.cpu().numpy()
            best_idx = np.argmax(confidences)
            
            # è·å–è¾¹ç•Œæ¡†åæ ‡ (x1, y1, x2, y2)
            box = boxes.xyxy[best_idx].cpu().numpy()
            x1, y1, x2, y2 = box
            
            # è½¬æ¢ä¸º (x, y, w, h) æ ¼å¼
            x = int(x1)
            y = int(y1)
            width = int(x2 - x1)
            height = int(y2 - y1)
            
            return (x, y, width, height)
        
        return None
    
    def _detect_face_opencv_dnn(self, frame: np.ndarray, h: int, w: int) -> Optional[Tuple[int, int, int, int]]:
        """ä½¿ç”¨OpenCV DNNæ£€æµ‹äººè„¸"""
        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123])
        self.face_net.setInput(blob)
        detections = self.face_net.forward()
        
        best_confidence = 0
        best_box = None
        
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                x1, y1, x2, y2 = box.astype(int)
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_box = (x1, y1, x2 - x1, y2 - y1)
        
        return best_box
    
    def _detect_face_haar_cascade(self, frame: np.ndarray, h: int, w: int) -> Optional[Tuple[int, int, int, int]]:
        """ä½¿ç”¨ä¼ ç»ŸHaarçº§è”æ£€æµ‹äººè„¸"""
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        
        if len(faces) > 0:
            # è¿”å›æœ€å¤§çš„äººè„¸
            areas = [face_w * face_h for (x, y, face_w, face_h) in faces]
            max_idx = np.argmax(areas)
            return tuple(faces[max_idx])
        
        return None
    
    def calculate_batch_metrics(self, 
                               pred_dir: str, 
                               gt_dir: Optional[str] = None,
                               pattern: str = "*.mp4") -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è®¡ç®—è§†é¢‘æŒ‡æ ‡
        
        Args:
            pred_dir: é¢„æµ‹è§†é¢‘ç›®å½•
            gt_dir: çœŸå€¼è§†é¢‘ç›®å½• (å¯é€‰)
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            æŒ‡æ ‡ç»“æœåˆ—è¡¨
        """
        
        # è·å–é¢„æµ‹è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        pred_files = sorted(glob.glob(os.path.join(pred_dir, pattern)))
        if not pred_files:
            print(f"âš ï¸ åœ¨ {pred_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é… {pattern} çš„æ–‡ä»¶")
            return []
        
        print(f"ğŸ” æ‰¾åˆ° {len(pred_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        results = []
        
        for pred_file in tqdm(pred_files, desc="è®¡ç®—æŒ‡æ ‡"):
            pred_name = os.path.basename(pred_file)
            
            # æŸ¥æ‰¾å¯¹åº”çš„çœŸå€¼è§†é¢‘
            gt_file = None
            if gt_dir:
                # å°è¯•å‡ ç§å¯èƒ½çš„å‘½åæ–¹å¼
                possible_names = [
                    pred_name,
                    pred_name.replace('_ta2v', ''),
                    pred_name.replace('_ia2v', ''),
                    pred_name.split('_')[0] + '.mp4'
                ]
                
                for name in possible_names:
                    gt_path = os.path.join(gt_dir, name)
                    if os.path.exists(gt_path):
                        gt_file = gt_path
                        break
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self.calculate_video_metrics(pred_file, gt_file)
            results.append(metrics)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶ï¼ŒåŒ…å«å¹³å‡å€¼ç»Ÿè®¡"""
        
        # è®¡ç®—å¹³å‡å€¼
        if results:
            average_metrics = self._calculate_average_metrics(results)
            
            # åˆ›å»ºåŒ…å«å¹³å‡å€¼çš„å®Œæ•´ç»“æœ
            output_data = {
                "summary": {
                    "total_videos": len(results),
                    "successful_videos": sum(1 for r in results if r.get('error') is None),
                    "average_metrics": average_metrics
                },
                "individual_results": results
            }
        else:
            output_data = {
                "summary": {
                    "total_videos": 0,
                    "successful_videos": 0,
                    "average_metrics": {}
                },
                "individual_results": []
            }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        # æ‰“å°å¹³å‡å€¼æ‘˜è¦
        if results and average_metrics:
            print(f"\nğŸ“Š å¹³å‡å€¼æ‘˜è¦:")
            for category, metrics in average_metrics.items():
                if metrics:
                    print(f"  {category}:")
                    for metric, value in metrics.items():
                        if isinstance(value, float):
                            print(f"    {metric}: {value:.4f}")
                        else:
                            print(f"    {metric}: {value}")
    
    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡çš„å¹³å‡å€¼"""
        
        # å®šä¹‰æŒ‡æ ‡åˆ†ç»„
        metric_groups = {
            "åŸºæœ¬ä¿¡æ¯": ['frame_count', 'width', 'height', 'fps', 'duration_seconds'],
            "å›¾åƒç»Ÿè®¡": ['mean_brightness', 'mean_contrast', 'mean_saturation', 'sharpness_score'],
            "äººè„¸åˆ†æ": ['face_detection_rate', 'avg_face_size', 'face_stability'],
            "è¿åŠ¨åˆ†æ": ['motion_intensity', 'frame_difference'],
            "LSEæŒ‡æ ‡": ['lse_distance', 'lse_confidence'],
            "VBenchæŒ‡æ ‡": ['subject_consistency', 'background_consistency', 'motion_smoothness',
                         'dynamic_degree', 'aesthetic_quality', 'imaging_quality'],
            "å¯¹æ¯”æŒ‡æ ‡": ['face_psnr', 'face_ssim', 'face_lpips']
        }
        
        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
        successful_results = [r for r in results if r.get('error') is None]
        
        if not successful_results:
            return {}
        
        average_metrics = {}
        
        for group_name, metrics in metric_groups.items():
            group_averages = {}
            
            for metric in metrics:
                # æ”¶é›†æœ‰æ•ˆçš„æ•°å€¼
                values = []
                for result in successful_results:
                    value = result.get(metric)
                    if value is not None and isinstance(value, (int, float)) and not np.isnan(value):
                        values.append(value)
                
                # è®¡ç®—å¹³å‡å€¼
                if values:
                    group_averages[metric] = np.mean(values)
            
            if group_averages:
                average_metrics[group_name] = group_averages
        
        return average_metrics
    
    def print_summary_stats(self, results: List[Dict[str, Any]]):
        """æ‰“å°æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        
        if not results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç»Ÿè®¡")
            return
        
        print(f"\nğŸ“Š æŒ‡æ ‡æ±‡æ€»ç»Ÿè®¡ ({len(results)} ä¸ªè§†é¢‘)")
        print("=" * 60)
        
        # ç»Ÿè®¡æˆåŠŸç‡
        total_videos = len(results)
        successful_videos = sum(1 for r in results if r.get('error') is None)
        
        print(f"è§†é¢‘æ€»æ•°: {total_videos}")
        print(f"æˆåŠŸå¤„ç†: {successful_videos}")
        print(f"æˆåŠŸç‡: {successful_videos/total_videos:.2%}")
        
        # ç»Ÿè®¡å„ç±»æŒ‡æ ‡
        stats = {}
        
        metric_keys = ['face_psnr', 'face_ssim', 'face_lpips', 
                      'lse_distance', 'lse_confidence',
                      'subject_consistency', 'background_consistency', 'motion_smoothness',
                      'dynamic_degree', 'aesthetic_quality', 'imaging_quality']
        
        for key in metric_keys:
            values = [r[key] for r in results if r.get(key) is not None and r.get('error') is None]
            if values:
                stats[key] = {
                    'count': len(values),
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if stats:
            print(f"\nğŸ“ˆ æŒ‡æ ‡ç»Ÿè®¡:")
            
            # åˆ†ç»„æ˜¾ç¤º
            print(f"\nğŸ” äººè„¸å¯¹æ¯”æŒ‡æ ‡:")
            for key in ['face_psnr', 'face_ssim', 'face_lpips']:
                if key in stats:
                    stat = stats[key]
                    print(f"  {key}:")
                    print(f"    æ ·æœ¬æ•°: {stat['count']}")
                    print(f"    å‡å€¼: {stat['mean']:.4f}")
                    print(f"    æ ‡å‡†å·®: {stat['std']:.4f}")
                    print(f"    èŒƒå›´: [{stat['min']:.4f}, {stat['max']:.4f}]")
            
            print(f"\nğŸµ LSEæŒ‡æ ‡:")
            for key in ['lse_distance', 'lse_confidence']:
                if key in stats:
                    stat = stats[key]
                    print(f"  {key}:")
                    print(f"    æ ·æœ¬æ•°: {stat['count']}")
                    print(f"    å‡å€¼: {stat['mean']:.4f}")
                    print(f"    æ ‡å‡†å·®: {stat['std']:.4f}")
                    print(f"    èŒƒå›´: [{stat['min']:.4f}, {stat['max']:.4f}]")
            
            print(f"\nğŸ”¥ VBenchæŒ‡æ ‡:")
            vbench_keys = ['subject_consistency', 'background_consistency', 'motion_smoothness',
                          'dynamic_degree', 'aesthetic_quality', 'imaging_quality']
            for key in vbench_keys:
                if key in stats:
                    stat = stats[key]
                    print(f"  {key}:")
                    print(f"    æ ·æœ¬æ•°: {stat['count']}")
                    print(f"    å‡å€¼: {stat['mean']:.4f}")
                    print(f"    æ ‡å‡†å·®: {stat['std']:.4f}")
                    print(f"    èŒƒå›´: [{stat['min']:.4f}, {stat['max']:.4f}]")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'vbench_calculator') and self.vbench_calculator:
            print("ğŸ—‘ï¸ æ¸…ç†VBenchèµ„æº...")
            try:
                self.vbench_calculator.cleanup()
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†VBenchèµ„æºå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç»¼åˆè§†é¢‘æŒ‡æ ‡è®¡ç®—å™¨")
    parser.add_argument("--pred_dir", type=str, required=True, help="é¢„æµ‹è§†é¢‘ç›®å½•")
    parser.add_argument("--gt_dir", type=str, help="çœŸå€¼è§†é¢‘ç›®å½•")
    parser.add_argument("--output", type=str, default="metrics_results.json", help="è¾“å‡ºJSONæ–‡ä»¶")
    parser.add_argument("--pattern", type=str, default="*.mp4", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"], help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--vbench", action="store_true", help="å¯ç”¨VBenchæŒ‡æ ‡è®¡ç®—")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨ç»¼åˆè§†é¢‘æŒ‡æ ‡è®¡ç®—å™¨")
    
    # åˆ›å»ºè®¡ç®—å™¨
    calculator = VideoMetricsCalculator(device=args.device, enable_vbench=args.vbench)
    
    try:
        # æ‰¹é‡è®¡ç®—æŒ‡æ ‡
        results = calculator.calculate_batch_metrics(
            pred_dir=args.pred_dir,
            gt_dir=args.gt_dir,
            pattern=args.pattern
        )
        
        # ä¿å­˜ç»“æœ
        calculator.save_results(results, args.output)
        
        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        calculator.print_summary_stats(results)
        
        print("\nğŸ‰ è®¡ç®—å®Œæˆ!")
        
    finally:
        # æ¸…ç†èµ„æº
        calculator.cleanup()


if __name__ == "__main__":
    main() 