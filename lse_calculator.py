#!/usr/bin/env python3
"""
LSE (Lip-Sync Error) Calculator
åŸºäºSyncNetçš„Python APIï¼Œè®¡ç®—LSE-D (Distance) å’Œ LSE-C (Confidence) æŒ‡æ ‡

Copyright (c) 2025 Fating Hong <fatinghong@gmail.com>
All rights reserved.

This module provides Python API for SyncNet-based lip-sync error calculation.

ä½¿ç”¨æ–¹æ³•:
    from evalutation.lse_calculator import LSECalculator
    
    calculator = LSECalculator()
    lse_distance, lse_confidence = calculator.calculate_single_video("video.mp4")
"""

import os
import sys
import time
import cv2
import numpy as np
import torch
import subprocess
import tempfile
import shutil
import glob
import pickle
from pathlib import Path
from typing import Tuple, List, Optional, Dict
import python_speech_features
from scipy.io import wavfile
from scipy import signal
from scipy.interpolate import interp1d
import scenedetect
from scenedetect.video_manager import VideoManager
from scenedetect.scene_manager import SceneManager
from scenedetect.frame_timecode import FrameTimecode
from scenedetect.stats_manager import StatsManager
from scenedetect.detectors import ContentDetector

# å¯¼å…¥æœ¬åœ°çš„SyncNetæ¨¡å—
try:
    # ä½œä¸ºåŒ…å¯¼å…¥æ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from .syncnet_core.model import S
    from .syncnet_core.detectors import S3FD
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)
    from syncnet_core.model import S
    from syncnet_core.detectors import S3FD


class LSECalculator:
    """LSEè®¡ç®—å™¨"""
    
    def __init__(self, 
                 model_path: Optional[str] = None,
                 device: str = "cuda",
                 batch_size: int = 20,
                 vshift: int = 15,
                 facedet_scale: float = 0.25,
                 crop_scale: float = 0.40,
                 min_track: int = 100,
                 frame_rate: int = 25,
                 num_failed_det: int = 25,
                 min_face_size: int = 100):
        """
        åˆå§‹åŒ–LSEè®¡ç®—å™¨
        
        Args:
            model_path: SyncNetæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ("cuda" æˆ– "cpu")
            batch_size: æ‰¹å¤„ç†å¤§å°
            vshift: è§†é¢‘åç§»èŒƒå›´
            facedet_scale: äººè„¸æ£€æµ‹ç¼©æ”¾å› å­
            crop_scale: è£å‰ªç¼©æ”¾å› å­
            min_track: æœ€å°è·Ÿè¸ªæŒç»­æ—¶é—´
            frame_rate: å¸§ç‡
            num_failed_det: å…è®¸çš„æ£€æµ‹å¤±è´¥æ¬¡æ•°
            min_face_size: æœ€å°äººè„¸å¤§å°(åƒç´ )
        """
        self.device = device
        self.batch_size = batch_size
        self.vshift = vshift
        self.facedet_scale = facedet_scale
        self.crop_scale = crop_scale
        self.min_track = min_track
        self.frame_rate = frame_rate
        self.num_failed_det = num_failed_det
        self.min_face_size = min_face_size
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            model_path = os.path.join(current_dir, "models", "syncnet_v2.model")
        
        # åˆå§‹åŒ–SyncNetæ¨¡å‹
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½SyncNetæ¨¡å‹: {model_path}")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"SyncNetæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
        self.syncnet = S(num_layers_in_fc_layers=1024)
        if self.device == "cuda" and torch.cuda.is_available():
            self.syncnet = self.syncnet.cuda()
        else:
            self.device = "cpu"
            
        # åŠ è½½æ¨¡å‹å‚æ•°
        loaded_state = torch.load(model_path, map_location=lambda storage, loc: storage)
        self_state = self.syncnet.state_dict()
        for name, param in loaded_state.items():
            self_state[name].copy_(param)
        self.syncnet.eval()
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨")
        try:
            self.face_detector = S3FD(device=self.device)
            print(f"âœ… LSEè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def calculate_single_video(self, video_path: str, verbose: bool = True) -> Tuple[Optional[float], Optional[float]]:
        """
        è®¡ç®—å•ä¸ªè§†é¢‘çš„LSEæŒ‡æ ‡
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            (lse_distance, lse_confidence): LSEè·ç¦»å’Œç½®ä¿¡åº¦
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
        if verbose:
            print(f"ğŸ¬ è®¡ç®—è§†é¢‘LSE: {os.path.basename(video_path)}")
            
        start_time = time.time()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                # æ­¥éª¤1: é¢„å¤„ç†è§†é¢‘
                if verbose:
                    print("  ğŸ“ æ­¥éª¤1: é¢„å¤„ç†è§†é¢‘...")
                    
                preprocessed_videos = self._preprocess_video(video_path, temp_dir, verbose)
                
                if not preprocessed_videos:
                    if verbose:
                        print("  âš ï¸  æ— æ³•æå–æœ‰æ•ˆçš„äººè„¸ç‰‡æ®µ")
                    return None, None
                
                # æ­¥éª¤2: è®¡ç®—LSEåˆ†æ•°
                if verbose:
                    print(f"  ğŸ§® æ­¥éª¤2: è®¡ç®—LSEåˆ†æ•° ({len(preprocessed_videos)}ä¸ªç‰‡æ®µ)...")
                    
                distances, confidences = [], []
                
                for video_file in preprocessed_videos:
                    dist, conf = self._calculate_lse_for_clip(video_file, temp_dir, verbose)
                    if dist is not None and conf is not None:
                        distances.append(dist)
                        confidences.append(conf)
                
                if not distances:
                    if verbose:
                        print("  âš ï¸  æ— æ³•è®¡ç®—LSEåˆ†æ•°")
                    return None, None
                
                # è®¡ç®—å¹³å‡å€¼
                avg_distance = np.mean(distances)
                avg_confidence = np.mean(confidences)
                
                elapsed = time.time() - start_time
                if verbose:
                    print(f"  âœ… LSEè®¡ç®—å®Œæˆ ({elapsed:.2f}s)")
                    print(f"     LSEè·ç¦»: {avg_distance:.4f}")
                    print(f"     LSEç½®ä¿¡åº¦: {avg_confidence:.4f}")
                
                return avg_distance, avg_confidence
                
            except Exception as e:
                if verbose:
                    print(f"  âŒ LSEè®¡ç®—å¤±è´¥: {e}")
                return None, None
    
    def calculate_batch(self, video_paths: List[str], verbose: bool = True) -> Dict[str, Tuple[Optional[float], Optional[float]]]:
        """
        æ‰¹é‡è®¡ç®—å¤šä¸ªè§†é¢‘çš„LSEæŒ‡æ ‡
        
        Args:
            video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºè§†é¢‘è·¯å¾„ï¼Œå€¼ä¸º(lse_distance, lse_confidence)
        """
        results = {}
        
        if verbose:
            print(f"ğŸš€ å¼€å§‹æ‰¹é‡LSEè®¡ç®— ({len(video_paths)}ä¸ªè§†é¢‘)")
        
        for i, video_path in enumerate(video_paths, 1):
            if verbose:
                print(f"\n[{i}/{len(video_paths)}] å¤„ç†: {os.path.basename(video_path)}")
            
            try:
                lse_d, lse_c = self.calculate_single_video(video_path, verbose=verbose)
                results[video_path] = (lse_d, lse_c)
            except Exception as e:
                if verbose:
                    print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                results[video_path] = (None, None)
        
        if verbose:
            print(f"\nâœ… æ‰¹é‡è®¡ç®—å®Œæˆ")
            success_count = sum(1 for v in results.values() if v[0] is not None)
            print(f"   æˆåŠŸ: {success_count}/{len(video_paths)}")
        
        return results
    
    def _preprocess_video(self, video_path: str, work_dir: str, verbose: bool = False) -> List[str]:
        """é¢„å¤„ç†è§†é¢‘ï¼Œæå–äººè„¸ç‰‡æ®µ"""
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        reference = "temp_video"
        avi_dir = os.path.join(work_dir, "pyavi", reference)
        frames_dir = os.path.join(work_dir, "pyframes", reference)
        crop_dir = os.path.join(work_dir, "pycrop", reference)
        work_subdir = os.path.join(work_dir, "pywork", reference)
        tmp_dir = os.path.join(work_dir, "pytmp", reference)
        
        os.makedirs(avi_dir, exist_ok=True)
        os.makedirs(frames_dir, exist_ok=True)
        os.makedirs(crop_dir, exist_ok=True)
        os.makedirs(work_subdir, exist_ok=True)
        os.makedirs(tmp_dir, exist_ok=True)
        
        # è½¬æ¢è§†é¢‘æ ¼å¼å’Œæå–å¸§
        video_avi = os.path.join(avi_dir, "video.avi")
        audio_wav = os.path.join(avi_dir, "audio.wav")
        
        # è½¬æ¢è§†é¢‘
        cmd = f"ffmpeg -y -i {video_path} -qscale:v 2 -async 1 -r {self.frame_rate} {video_avi}"
        subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # æå–å¸§
        cmd = f"ffmpeg -y -i {video_avi} -qscale:v 2 -threads 1 -f image2 {frames_dir}/%06d.jpg"
        subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # æå–éŸ³é¢‘
        cmd = f"ffmpeg -y -i {video_avi} -ac 1 -vn -acodec pcm_s16le -ar 16000 {audio_wav}"
        subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # äººè„¸æ£€æµ‹
        if verbose:
            print("    ğŸ” æ£€æµ‹äººè„¸...")
        faces = self._detect_faces(frames_dir)
        
        # åœºæ™¯æ£€æµ‹
        if verbose:
            print("    ğŸ¬ æ£€æµ‹åœºæ™¯...")
        scenes = self._detect_scenes(video_avi)
        
        # äººè„¸è·Ÿè¸ª
        if verbose:
            print("    ğŸ‘¤ è·Ÿè¸ªäººè„¸...")
        tracks = []
        for scene in scenes:
            if scene[1].frame_num - scene[0].frame_num >= self.min_track:
                scene_faces = faces[scene[0].frame_num:scene[1].frame_num]
                tracks.extend(self._track_faces(scene_faces))
        
        # è£å‰ªäººè„¸è§†é¢‘
        if verbose:
            print("    âœ‚ï¸  è£å‰ªäººè„¸è§†é¢‘...")
        cropped_videos = []
        for ii, track in enumerate(tracks):
            output_path = os.path.join(crop_dir, f"{ii:05d}.avi")
            if self._crop_face_video(track, frames_dir, audio_wav, output_path):
                cropped_videos.append(output_path)
        
        return cropped_videos
    
    def _detect_faces(self, frames_dir: str) -> List[List[Dict]]:
        """æ£€æµ‹äººè„¸"""
        flist = sorted(glob.glob(os.path.join(frames_dir, "*.jpg")))
        
        dets = []
        for fidx, fname in enumerate(flist):
            image = cv2.imread(fname)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            bboxes = self.face_detector.detect_faces(image_rgb, conf_th=0.9, scales=[self.facedet_scale])
            
            frame_dets = []
            for bbox in bboxes:
                frame_dets.append({
                    'frame': fidx, 
                    'bbox': bbox[:-1].tolist(), 
                    'conf': bbox[-1]
                })
            dets.append(frame_dets)
        
        return dets
    
    def _detect_scenes(self, video_path: str) -> List[Tuple]:
        """æ£€æµ‹åœºæ™¯"""
        video_manager = VideoManager([video_path])
        stats_manager = StatsManager()
        scene_manager = SceneManager(stats_manager)
        scene_manager.add_detector(ContentDetector())
        base_timecode = video_manager.get_base_timecode()
        
        video_manager.set_downscale_factor()
        video_manager.start()
        scene_manager.detect_scenes(frame_source=video_manager)
        scene_list = scene_manager.get_scene_list(base_timecode)
        
        if not scene_list:
            scene_list = [(video_manager.get_base_timecode(), video_manager.get_current_timecode())]
        
        return scene_list
    
    def _track_faces(self, scene_faces: List[List[Dict]]) -> List[Dict]:
        """è·Ÿè¸ªäººè„¸"""
        iou_threshold = 0.5
        tracks = []
        
        while True:
            track = []
            for frame_faces in scene_faces:
                for face in frame_faces:
                    if not track:
                        track.append(face)
                        frame_faces.remove(face)
                    elif face['frame'] - track[-1]['frame'] <= self.num_failed_det:
                        iou = self._calculate_iou(face['bbox'], track[-1]['bbox'])
                        if iou > iou_threshold:
                            track.append(face)
                            frame_faces.remove(face)
                            continue
                    else:
                        break
            
            if not track:
                break
            elif len(track) > self.min_track:
                # æ’å€¼è½¨è¿¹
                framenum = np.array([f['frame'] for f in track])
                bboxes = np.array([np.array(f['bbox']) for f in track])
                
                frame_i = np.arange(framenum[0], framenum[-1] + 1)
                
                bboxes_i = []
                for ij in range(4):
                    interpfn = interp1d(framenum, bboxes[:, ij])
                    bboxes_i.append(interpfn(frame_i))
                bboxes_i = np.stack(bboxes_i, axis=1)
                
                # æ£€æŸ¥äººè„¸å¤§å°
                face_width = np.mean(bboxes_i[:, 2] - bboxes_i[:, 0])
                face_height = np.mean(bboxes_i[:, 3] - bboxes_i[:, 1])
                
                if max(face_width, face_height) > self.min_face_size:
                    tracks.append({'frame': frame_i, 'bbox': bboxes_i})
        
        return tracks
    
    def _calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """è®¡ç®—IoU"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        inter_area = max(0, x2 - x1) * max(0, y2 - y1)
        
        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
        
        iou = inter_area / float(box1_area + box2_area - inter_area)
        return iou
    
    def _crop_face_video(self, track: Dict, frames_dir: str, audio_path: str, output_path: str) -> bool:
        """è£å‰ªäººè„¸è§†é¢‘"""
        try:
            flist = sorted(glob.glob(os.path.join(frames_dir, "*.jpg")))
            
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            temp_video = output_path + "t.avi"
            vOut = cv2.VideoWriter(temp_video, fourcc, self.frame_rate, (224, 224))
            
            dets = {'x': [], 'y': [], 's': []}
            
            for det in track['bbox']:
                dets['s'].append(max((det[3] - det[1]), (det[2] - det[0])) / 2)
                dets['y'].append((det[1] + det[3]) / 2)
                dets['x'].append((det[0] + det[2]) / 2)
            
            # å¹³æ»‘æ£€æµ‹ç»“æœ
            dets['s'] = signal.medfilt(dets['s'], kernel_size=13)
            dets['x'] = signal.medfilt(dets['x'], kernel_size=13)
            dets['y'] = signal.medfilt(dets['y'], kernel_size=13)
            
            for fidx, frame in enumerate(track['frame']):
                cs = self.crop_scale
                bs = dets['s'][fidx]
                bsi = int(bs * (1 + 2 * cs))
                
                image = cv2.imread(flist[frame])
                frame_padded = np.pad(image, ((bsi, bsi), (bsi, bsi), (0, 0)), 'constant', constant_values=(110, 110))
                
                my = dets['y'][fidx] + bsi
                mx = dets['x'][fidx] + bsi
                
                face = frame_padded[int(my - bs):int(my + bs * (1 + 2 * cs)),
                                   int(mx - bs * (1 + cs)):int(mx + bs * (1 + cs))]
                
                vOut.write(cv2.resize(face, (224, 224)))
            
            vOut.release()
            
            # è£å‰ªéŸ³é¢‘
            audiostart = track['frame'][0] / self.frame_rate
            audioend = (track['frame'][-1] + 1) / self.frame_rate
            
            temp_audio = output_path.replace('.avi', '_audio.wav')
            cmd = f"ffmpeg -y -i {audio_path} -ss {audiostart:.3f} -to {audioend:.3f} {temp_audio}"
            subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # åˆå¹¶éŸ³è§†é¢‘
            cmd = f"ffmpeg -y -i {temp_video} -i {temp_audio} -c:v copy -c:a copy {output_path}"
            result = subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video):
                os.remove(temp_video)
            if os.path.exists(temp_audio):
                os.remove(temp_audio)
            
            return result == 0
            
        except Exception as e:
            print(f"è£å‰ªè§†é¢‘å¤±è´¥: {e}")
            return False
    
    def _calculate_lse_for_clip(self, video_path: str, work_dir: str, verbose: bool = False) -> Tuple[Optional[float], Optional[float]]:
        """è®¡ç®—å•ä¸ªè§†é¢‘ç‰‡æ®µçš„LSE"""
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                # æå–å¸§å’ŒéŸ³é¢‘
                frame_pattern = os.path.join(temp_dir, "%06d.jpg")
                audio_path = os.path.join(temp_dir, "audio.wav")
                
                # æå–å¸§
                cmd = f"ffmpeg -loglevel error -y -i {video_path} -threads 1 -f image2 {frame_pattern}"
                subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                # æå–éŸ³é¢‘
                cmd = f"ffmpeg -loglevel error -y -i {video_path} -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 {audio_path}"
                subprocess.call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                # åŠ è½½è§†é¢‘å¸§
                images = []
                flist = sorted(glob.glob(os.path.join(temp_dir, "*.jpg")))
                
                for fname in flist:
                    img_input = cv2.imread(fname)
                    img_input = cv2.resize(img_input, (224, 224))
                    images.append(img_input)
                
                if len(images) < 5:
                    return None, None
                
                im = np.stack(images, axis=3)
                im = np.expand_dims(im, axis=0)
                im = np.transpose(im, (0, 3, 4, 1, 2))
                imtv = torch.autograd.Variable(torch.from_numpy(im.astype(float)).float())
                
                # åŠ è½½éŸ³é¢‘
                if not os.path.exists(audio_path):
                    return None, None
                
                sample_rate, audio = wavfile.read(audio_path)
                mfcc = zip(*python_speech_features.mfcc(audio, sample_rate))
                mfcc = np.stack([np.array(i) for i in mfcc])
                
                cc = np.expand_dims(np.expand_dims(mfcc, axis=0), axis=0)
                cct = torch.autograd.Variable(torch.from_numpy(cc.astype(float)).float())
                
                # æ£€æŸ¥é•¿åº¦
                min_length = min(len(images), len(audio) // 640)
                lastframe = min_length - 5
                
                if lastframe <= 0:
                    return None, None
                
                # æå–ç‰¹å¾
                im_feat = []
                cc_feat = []
                
                for i in range(0, lastframe, self.batch_size):
                    # è§†é¢‘ç‰¹å¾
                    im_batch = [imtv[:, :, vframe:vframe + 5, :, :] 
                               for vframe in range(i, min(lastframe, i + self.batch_size))]
                    im_in = torch.cat(im_batch, 0)
                    
                    if self.device == "cuda":
                        im_in = im_in.cuda()
                    
                    im_out = self.syncnet.forward_lip(im_in)
                    im_feat.append(im_out.data.cpu())
                    
                    # éŸ³é¢‘ç‰¹å¾
                    cc_batch = [cct[:, :, :, vframe * 4:vframe * 4 + 20] 
                               for vframe in range(i, min(lastframe, i + self.batch_size))]
                    cc_in = torch.cat(cc_batch, 0)
                    
                    if self.device == "cuda":
                        cc_in = cc_in.cuda()
                    
                    cc_out = self.syncnet.forward_aud(cc_in)
                    cc_feat.append(cc_out.data.cpu())
                
                im_feat = torch.cat(im_feat, 0)
                cc_feat = torch.cat(cc_feat, 0)
                
                # è®¡ç®—è·ç¦»
                dists = self._calc_pdist(im_feat, cc_feat, vshift=self.vshift)
                mdist = torch.mean(torch.stack(dists, 1), 1)
                
                minval, minidx = torch.min(mdist, 0)
                conf = torch.median(mdist) - minval
                
                return minval.numpy().item(), conf.numpy().item()
                
        except Exception as e:
            if verbose:
                print(f"è®¡ç®—LSEå¤±è´¥: {e}")
            return None, None
    
    def _calc_pdist(self, feat1: torch.Tensor, feat2: torch.Tensor, vshift: int = 10) -> List[torch.Tensor]:
        """è®¡ç®—ç‰¹å¾è·ç¦»"""
        win_size = vshift * 2 + 1
        feat2p = torch.nn.functional.pad(feat2, (0, 0, vshift, vshift))
        
        dists = []
        for i in range(len(feat1)):
            dists.append(torch.nn.functional.pairwise_distance(
                feat1[[i], :].repeat(win_size, 1), 
                feat2p[i:i + win_size, :]
            ))
        
        return dists


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LSEè®¡ç®—å™¨")
    parser.add_argument("--video", type=str, required=True, help="è¾“å…¥è§†é¢‘æ–‡ä»¶")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"], help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®¡ç®—å™¨
    calculator = LSECalculator(device=args.device)
    
    # è®¡ç®—LSE
    lse_d, lse_c = calculator.calculate_single_video(args.video, verbose=True)
    
    if lse_d is not None and lse_c is not None:
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   LSE Distance: {lse_d:.4f}")
        print(f"   LSE Confidence: {lse_c:.4f}")
    else:
        print(f"\nâŒ LSEè®¡ç®—å¤±è´¥")


if __name__ == "__main__":
    main() 